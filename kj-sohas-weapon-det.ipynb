{"cells":[{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.11.0\n","0.15.2a0\n"]}],"source":["import torch\n","import torchvision\n","\n","print(torch.__version__)\n","print(torchvision.__version__)"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-16T07:23:29.661501Z","iopub.status.busy":"2023-09-16T07:23:29.660357Z","iopub.status.idle":"2023-09-16T07:23:30.170729Z","shell.execute_reply":"2023-09-16T07:23:30.169404Z","shell.execute_reply.started":"2023-09-16T07:23:29.661462Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T07:23:30.173516Z","iopub.status.busy":"2023-09-16T07:23:30.172997Z","iopub.status.idle":"2023-09-16T07:23:41.232999Z","shell.execute_reply":"2023-09-16T07:23:41.231497Z","shell.execute_reply.started":"2023-09-16T07:23:30.173482Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"]}],"source":["import tensorflow as tf\n","import os\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","# print(gpus)\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)\n","    print(gpu)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T07:23:41.236026Z","iopub.status.busy":"2023-09-16T07:23:41.235222Z","iopub.status.idle":"2023-09-16T07:23:41.244737Z","shell.execute_reply":"2023-09-16T07:23:41.243616Z","shell.execute_reply.started":"2023-09-16T07:23:41.235990Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","# root_input_dir = '/kaggle/input'\n","# train_data_dir = validation_data_dir = test_data_dir = None\n","\n","# train_data_dir = f'{root_input_dir}/sohas-wepon-classification/Sohas_weapon-Classification/'\n","# # validation_data_dir = f'{root_dir}/intel-image/val'\n","# # test_data_dir = f'{root_dir}/intel-image/val'\n","\n","# def get_data(ims=150, batch_size=32):\n","    \n","#     augdict = {\n","#         'rotation_range':20,\n","#         'width_shift_range':0.2,\n","#         'height_shift_range':0.2,\n","#         'shear_range':0.2,\n","#         'zoom_range':0.2,\n","#         'horizontal_flip':True,\n","#         'fill_mode':'nearest',\n","#     }\n","\n","#     split = 0.2\n","\n","#     datagen = ImageDataGenerator(\n","#     #     rescale=1.0 / 255,\n","#         # **augdict,\n","#         validation_split=split\n","#     )\n","\n","#     auggen = ImageDataGenerator(\n","#     #     rescale=1.0 / 255,\n","#         **augdict,\n","#         validation_split=split\n","#     )\n","\n","#     valgen = ImageDataGenerator(\n","#     #     rescale=1.0 / 255,\n","#         # **augdict,\n","#         validation_split=split\n","#     )\n","#     testgen = ImageDataGenerator(\n","#     #     rescale=1.0 / 255,\n","#         # **augdict,\n","#     )\n","\n","\n","#     print(end='data_generator:\\t')\n","#     data_generator = None\n","#     data_generator = datagen.flow_from_directory(\n","#         train_data_dir,\n","#         target_size=(ims, ims),\n","#         batch_size=batch_size,\n","#         class_mode='categorical',\n","#         subset='training',  # Specify 'training' or 'validation'\n","#         shuffle=True\n","#     )\n","    \n","#     print(end='aug_generator:\\t')\n","#     aug_generator = None\n","#     aug_generator = auggen.flow_from_directory(\n","#         train_data_dir,\n","#         target_size=(ims, ims),\n","#         batch_size=batch_size,\n","#         class_mode='categorical',\n","#         subset='training',  # Specify 'training' or 'validation'\n","#         shuffle=True\n","#     )\n","\n","#     print(end='val_generator:\\t')\n","#     val_generator = None\n","#     val_generator = datagen.flow_from_directory(\n","#         train_data_dir,\n","#         target_size=(ims, ims),\n","#         batch_size=batch_size,\n","#         class_mode='categorical',\n","#         subset='validation',  # Specify 'training' or 'validation'\n","#         shuffle=False\n","#     )\n","\n","#     test_generator = None\n","#     if test_data_dir:\n","#         print(end='test_generator:\\t')\n","#         test_generator = testgen.flow_from_directory(\n","#             test_data_dir,\n","#             target_size=(ims, ims),\n","#             batch_size=batch_size,\n","#             class_mode='categorical',\n","#             shuffle=False\n","#         )\n","\n","#     num_classes, nb_train_samples, nb_validation_samples = val_generator.num_classes, len(data_generator.classes), len(val_generator.classes)\n","#     return data_generator, aug_generator, val_generator, test_generator, ims, batch_size, num_classes, nb_train_samples, nb_validation_samples\n","\n","\n","# data_generator, aug_generator, val_generator, test_generator, ims, batch_size, num_classes, nb_train_samples, nb_validation_samples = get_data(224, 32)\n","# # nb_train_samples, nb_validation_samples"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T07:25:17.231992Z","iopub.status.busy":"2023-09-16T07:25:17.231181Z","iopub.status.idle":"2023-09-16T07:27:49.460641Z","shell.execute_reply":"2023-09-16T07:27:49.459615Z","shell.execute_reply.started":"2023-09-16T07:25:17.231938Z"},"trusted":true},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# !pip install supervision\n","import supervision as sv\n","\n","train_dir = 'obj_train_data/images/train/'\n","train_ann_dir = 'obj_train_data/labels/train/'\n","val_dir = 'obj_train_data/images/test/'\n","val_ann_dir = 'obj_train_data/labels/test/'\n","\n","\n","dataset = sv.DetectionDataset.from_yolo(\n","    images_directory_path=train_dir,\n","    annotations_directory_path=train_ann_dir,\n","    data_yaml_path='dataset.yaml')\n","\n","len(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-09-16T07:31:16.310784Z","iopub.status.busy":"2023-09-16T07:31:16.310220Z","iopub.status.idle":"2023-09-16T07:31:20.083654Z","shell.execute_reply":"2023-09-16T07:31:20.081383Z","shell.execute_reply.started":"2023-09-16T07:31:16.310731Z"},"trusted":true},"outputs":[],"source":["import supervision as sv\n","\n","SAMPLE_SIZE =16\n","SAMPLE_GRID_SIZE = (4,4)\n","SAMPLE_PLOT_SIZE = (16,16)\n","\n","image_names = list(dataset.images.keys())[:SAMPLE_SIZE]\n","\n","mask_annotator = sv.MaskAnnotator()\n","box_annotator = sv.BoxAnnotator()\n","\n","images = []\n","for image_name in image_names:\n","#     image_name = image_name.split('/')[-1]\n","    image = dataset.images[image_name]\n","    annotations = dataset.annotations[image_name]\n","    labels = [\n","        dataset.classes[class_id]\n","        for class_id\n","        in annotations.class_id]\n","    annotates_image = mask_annotator.annotate(\n","        scene=image.copy(),\n","        detections=annotations)\n","    annotates_image = box_annotator.annotate(\n","        scene=annotates_image,\n","        detections=annotations,\n","        labels=labels)\n","    images.append(annotates_image)\n","\n","sv.plot_images_grid(\n","    images=images,\n","    titles=image_names,\n","    grid_size=SAMPLE_GRID_SIZE,\n","    size=SAMPLE_PLOT_SIZE)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/jijnasu/miniforge3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/jijnasu/miniforge3/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n","  Referenced from: <E736BB25-B038-3AFE-BCF3-8026B5D2C7B6> /Users/jijnasu/miniforge3/lib/python3.10/site-packages/torchvision/image.so\n","  Expected in:     <7A536E8A-60F4-39F0-AE7A-79B04AC14B92> /Users/jijnasu/miniforge3/lib/python3.10/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n","  warn(\n"]}],"source":["from torchvision.datasets import VisionDataset\n","from torchvision import transforms\n","\n","class YOLOv5Dataset(VisionDataset):\n","    def __init__(self, train_dir, annotation_txt_dir, yaml_dir, transform=None):\n","        super(YOLOv5Dataset, self).__init__(train_dir, transform=transform)\n","        self.annotation_txt_dir = annotation_txt_dir\n","\n","        # Load YAML file to get class names\n","        with open(os.path.join(yaml_dir, 'dataset.yaml'), 'r') as file:\n","            dataset_config = yaml.safe_load(file)\n","        self.class_names = dataset_config['names']\n","\n","    def __getitem__(self, index):\n","        # Load image\n","        img_path = self.samples[index][0]\n","        img = Image.open(img_path).convert('RGB')\n","\n","        # Load YOLOv5 format annotations\n","        annotation_path = os.path.join(self.annotation_txt_dir, os.path.basename(img_path).replace('.jpg', '.txt'))\n","        with open(annotation_path, 'r') as file:\n","            lines = file.readlines()\n","\n","        # Process and parse annotations here to get bounding boxes and class labels\n","        # You will need to implement the parsing logic to match your annotation format\n","\n","        # Apply transformations if needed\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        # Return image and target (bounding boxes, labels)\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.samples)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'train_dir' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/Users/jijnasu/Workspace/OD-WeaponDetection/Weapons and similar handled objects/Sohas_weapon-Detection-YOLOv5/kj-sohas-weapon-det.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jijnasu/Workspace/OD-WeaponDetection/Weapons%20and%20similar%20handled%20objects/Sohas_weapon-Detection-YOLOv5/kj-sohas-weapon-det.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jijnasu/Workspace/OD-WeaponDetection/Weapons%20and%20similar%20handled%20objects/Sohas_weapon-Detection-YOLOv5/kj-sohas-weapon-det.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m416\u001b[39m, \u001b[39m416\u001b[39m)),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jijnasu/Workspace/OD-WeaponDetection/Weapons%20and%20similar%20handled%20objects/Sohas_weapon-Detection-YOLOv5/kj-sohas-weapon-det.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jijnasu/Workspace/OD-WeaponDetection/Weapons%20and%20similar%20handled%20objects/Sohas_weapon-Detection-YOLOv5/kj-sohas-weapon-det.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jijnasu/Workspace/OD-WeaponDetection/Weapons%20and%20similar%20handled%20objects/Sohas_weapon-Detection-YOLOv5/kj-sohas-weapon-det.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jijnasu/Workspace/OD-WeaponDetection/Weapons%20and%20similar%20handled%20objects/Sohas_weapon-Detection-YOLOv5/kj-sohas-weapon-det.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Create an instance of your custom YOLOv5Dataset\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jijnasu/Workspace/OD-WeaponDetection/Weapons%20and%20similar%20handled%20objects/Sohas_weapon-Detection-YOLOv5/kj-sohas-weapon-det.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m train_dataset \u001b[39m=\u001b[39m YOLOv5Dataset(train_dir, annotation_txt_dir, yaml_dir, transform\u001b[39m=\u001b[39mtransform)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jijnasu/Workspace/OD-WeaponDetection/Weapons%20and%20similar%20handled%20objects/Sohas_weapon-Detection-YOLOv5/kj-sohas-weapon-det.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Create a DataLoader for the training dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jijnasu/Workspace/OD-WeaponDetection/Weapons%20and%20similar%20handled%20objects/Sohas_weapon-Detection-YOLOv5/kj-sohas-weapon-det.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dir' is not defined"]}],"source":["import os\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","# Define image transformations\n","transform = transforms.Compose([\n","    transforms.Resize((416, 416)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Create an instance of your custom YOLOv5Dataset\n","train_dataset = YOLOv5Dataset(train_dir, annotation_txt_dir, yaml_dir, transform=transform)\n","\n","# Create a DataLoader for the training dataset\n","batch_size = 8\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from ultralytics import YOLO\n","\n","model = YOLO(\"yolov8m.pt\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","image 1/1 /Users/jijnasu/Workspace/OD-WeaponDetection/Weapons and similar handled objects/Sohas_weapon-Detection-YOLOv5/obj_train_data/images/train/ABbframe00166.jpg: 384x640 1 person, 1 couch, 1 potted plant, 1 cell phone, 323.1ms\n","Speed: 2.1ms preprocess, 323.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n"]}],"source":["results = model.predict('/Users/jijnasu/Workspace/OD-WeaponDetection/Weapons and similar handled objects/Sohas_weapon-Detection-YOLOv5/obj_train_data/images/train/ABbframe00166.jpg')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n"]}],"source":["print(len(results))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["result = results[0]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Object type: tensor([0.])\n","Coordinates: tensor([[ 771.6041,  191.9480, 1067.5435, 1075.1079]])\n","Probability: tensor([0.9408])\n"]}],"source":["box = result.boxes[0]\n","print(\"Object type:\", box.cls)\n","print(\"Coordinates:\", box.xyxy)\n","print(\"Probability:\", box.conf)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Object type: 0.0\n","Coordinates: [771.6040649414062, 191.947998046875, 1067.54345703125, 1075.10791015625]\n","Probability: 0.9407950043678284\n"]}],"source":["cords = box.xyxy[0].tolist()\n","class_id = box.cls[0].item()\n","conf = box.conf[0].item()\n","print(\"Object type:\", class_id)\n","print(\"Coordinates:\", cords)\n","print(\"Probability:\", conf)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Object type: person\n","Coordinates: [772, 192, 1068, 1075]\n","Probability: 0.94\n"]}],"source":["cords = box.xyxy[0].tolist()\n","cords = [round(x) for x in cords]\n","class_id = result.names[box.cls[0].item()]\n","conf = round(box.conf[0].item(), 2)\n","print(\"Object type:\", class_id)\n","print(\"Coordinates:\", cords)\n","print(\"Probability:\", conf)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Object type: person\n","Coordinates: [772, 192, 1068, 1075]\n","Probability: 0.94\n","---\n","Object type: potted plant\n","Coordinates: [120, 33, 485, 791]\n","Probability: 0.66\n","---\n","Object type: couch\n","Coordinates: [1, 746, 419, 1074]\n","Probability: 0.6\n","---\n","Object type: cell phone\n","Coordinates: [734, 399, 806, 488]\n","Probability: 0.34\n","---\n"]}],"source":["for box in result.boxes:\n","  class_id = result.names[box.cls[0].item()]\n","  cords = box.xyxy[0].tolist()\n","  cords = [round(x) for x in cords]\n","  conf = round(box.conf[0].item(), 2)\n","  print(\"Object type:\", class_id)\n","  print(\"Coordinates:\", cords)\n","  print(\"Probability:\", conf)\n","  print(\"---\")"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# train: sohas-weapon-detection-yolov5/images/images/train/\n","# val: sohas-weapon-detection-yolov5/images/images/test/\n","train: train/images/\n","val: test/images/\n","\n","# number of classes\n","nc: 6\n","\n","# class names\n","names: ['pistol', 'smartphone', 'knife', 'monedero', 'billete', 'tarjeta']\n"]}],"source":["!cat dataset.yaml"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from ultralytics import YOLO\n","\n","model = YOLO(\"yolov8m.pt\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Ultralytics YOLOv8.0.180 ðŸš€ Python-3.10.6 torch-2.0.1 MPS (Apple M1)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=dataset.yaml, epochs=2, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=mps, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train10\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n","  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n","  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n","  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n","  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n","  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n","  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n","  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n","  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n","  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n"," 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n"," 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n"," 22        [15, 18, 21]  1   3779170  ultralytics.nn.modules.head.Detect           [6, [192, 384, 576]]          \n","Model summary: 295 layers, 25859794 parameters, 25859778 gradients\n","\n","Transferred 475/475 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train10', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/jijnasu/Workspace/OD-WeaponDetection/Weapons and similar handled objects/Sohas_weapon-Detection-YOLOv5/datasets/train/labels.cache... 5002 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5002/5002 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/jijnasu/Workspace/OD-WeaponDetection/Weapons and similar handled objects/Sohas_weapon-Detection-YOLOv5/datasets/test/labels.cache... 857 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 857/857 [00:00<?, ?it/s]\n","Plotting labels to runs/detect/train10/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 0 dataloader workers\n","Logging results to \u001b[1mruns/detect/train10\u001b[0m\n","Starting training for 2 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","  0%|          | 0/313 [00:00<?, ?it/s]loc(\"mps_not_equal\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/75428952-3aa4-11ee-8b65-46d450270006/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":253:0)): error: 'anec.not_equal_zero' op Invalid configuration for the following reasons: Tensor dimensions N1D1C1H1W537600 are not within supported range, N[1-65536]D[1-16384]C[1-65536]H[1-16384]W[1-16384].\n","loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/75428952-3aa4-11ee-8b65-46d450270006/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.not_equal_zero' op Invalid configuration for the following reasons: Tensor dimensions N1D1C1H1W537600 are not within supported range, N[1-65536]D[1-16384]C[1-65536]H[1-16384]W[1-16384].\n","loc(\"mps_not_equal\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/75428952-3aa4-11ee-8b65-46d450270006/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":253:0)): error: 'anec.not_equal_zero' op Invalid configuration for the following reasons: Tensor dimensions N1D1C1H1W134400 are not within supported range, N[1-65536]D[1-16384]C[1-65536]H[1-16384]W[1-16384].\n","loc(\"mps_select\"(\"(mpsFileLoc): /AppleInternal/Library/BuildRoots/75428952-3aa4-11ee-8b65-46d450270006/Library/Caches/com.apple.xbs/Sources/MetalPerformanceShadersGraph/mpsgraph/MetalPerformanceShadersGraph/Core/Files/MPSGraphUtilities.mm\":294:0)): error: 'anec.not_equal_zero' op Invalid configuration for the following reasons: Tensor dimensions N1D1C1H1W134400 are not within supported range, N[1-65536]D[1-16384]C[1-65536]H[1-16384]W[1-16384].\n","        1/2         0G      1.073      5.972      1.452         37        640:   1%|â–         | 4/313 [06:34<8:53:48, 103.65s/it]"]}],"source":["model.train(data=\"dataset.yaml\", epochs=2, device='mps')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":4}
